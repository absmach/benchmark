# Report from Digitalocean managed [Kubernetes cluster](https://www.digitalocean.com/products/kubernetes/).

Cluster size: 3 Nodes CPU Optimized droplet - 8 vCPU 16 GB RAM

Estimated droplet cost's for cluster: $480/month

Mainflux services scaled to 3 instances:

* Mqtt adapter
* Authn
* Things
* Redis-things
* Envoy
* Nats


## 1-to-1 Scenario

Clients pool: 2500

Test durition: 5min

RPS: 1

QoS Level: 2

In 1-to-1 test 2500 clients subscribe to an exclusive Mainflux channel and the same clients send one message every second on that channel to themselves.
No message loss was detected.

**NOTE:** Graphs Time unit is microsecond.

![DO 1-to-1 2.5k QoS 2 Total connections](assets/DO/1-to-1-2.5k-qos2-totalconn.png "DO 1-to-1 2.5k QoS 2 Total connections")

*We see total number of connections during test is constant*

---

![DO 1-to-1 2.5k QoS 2 Total messages PRS](assets/DO/1-to-1-2.5k-qos2-totalmsg.png "DO 1-to-1 2.5k QoS 2 Total messages")
![DO 1-to-1 2.5k QoS 2 Total received messages](assets/DO/1-to-1-2.5k-qos2-totalmsg-rec.png "DO 1-to-1 2.5k QoS 2 Total received messages")

*We see total number of published and consumed messages is the same*

---

![DO 1-to-1 2.5k QoS 2 Total messages RPS](assets/DO/1-to-1-2.5k-qos2-totalmsg-rps.png "DO 1-to-1 2.5k QoS 2 Total messages RPS")

*We see total number of messages per second (RPS) is 2500 for both publishing and receiving*

---

![DO 1-to-1 2.5k QoS 2 Publish to Pubrec latency](assets/DO/1-to-1-2.5k-qos2-publishers-pub-pubrec-latency.png "DO 1-to-1 2.5k QoS 2 Pub to Sub latency")

*Pub to Pubrec latency in microseconds (100k microseconds are 100 milliseconds) *

---
> Useful facts: 
>* 95% of clients had latency from 89ms up to 93ms
>* Max latency was up to 440ms

---

![DO 1-to-1 2.5k QoS 2 Pub to Sub latency](assets/DO/1-to-1-2.5k-qos2-pub-sub-lat.png "DO 1-to-1 2.5k QoS 2 Pub to Sub latency")

*Pub to sub latency in microseconds (100k microseconds are 100 milliseconds) *

---
> Useful facts: 
>* 95% of clients had latency from 91ms up to 94ms
>* Max latency was up to 440ms

---


Results metrics
![DO 1-to-1 2.5k QoS 2 Publish reveived ACK latency](assets/DO/1-to-1-2.5k-qos2-results.png "DO 1-to-1 2.5k QoS 2  Publish reveived ACK")

*Results metrics are generated by mzbench tool*

---

### Kubernetes Cluster resources insights
![DO 1-to-1 2.5k QoS 2 - CPU usage on namespace](assets/DO/1-to-1-2.5k-qos2-cpu-usage-namespace.png "DO 1-to-1 2.5k QoS 2 - CPU usage on namespace")

*Kubernetes CPU usage in whole Mainflux namespace during testing*

---

![DO 1-to-1 2.5k QoS 2 - Memory usage on namespace](assets/DO/1-to-1-2.5k-qos2-mem-usage-namespace.png "DO 1-to-1 2.5k QoS 2 - Memory usage on namespace")

*Kubernetes Memory usage in whole Mainflux namespace during testing*

---

![DO 1-to-1 2.5k QoS 2 - CPU usage on MQTT statefulset](assets/DO/1-to-1-2.5k-qos2-cpu-mqtt-adapters.png "DO 1-to-1 2.5k QoS 2 - CPU usage on MQTT statefulset")

*Kubernetes CPU usage in MQTT statefulset during testing*

---

![DO 1-to-1 2.5k QoS 2 - Memory usage on MQTT statefulset](assets/DO/1-to-1-2.5k-qos2-mem-usage-mqtt.png "DO 1-to-1 2.5k QoS 2 - Memory usage on MQTT statefulset")

*Kubernetes Memory usage in MQTT statefulset namespace during testing*

---

![DO 1-to-1 2.5k QoS 2 - Networking on MQTT statefulset](assets/DO/1-to-1-2.5k-qos2-mem-usage-mqtt.png "DO 1-to-1 2.5k QoS 2 - Networking on MQTT statefulset")
*Kubernetes Network receive/transmit bandwidth in MQTT statefulset namespace during testing*

