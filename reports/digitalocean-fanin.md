# Report from Digitalocean managed [Kubernetes cluster](https://www.digitalocean.com/products/kubernetes/).

Cluster size: 3 Nodes CPU Optimized droplet - 8 vCPU 16 GB RAM

Estimated droplet cost's for cluster: $480/month

Mainflux services scaled to 3 instances:

* Mqtt adapter
* Authn
* Things
* Redis-things
* Envoy
* Nats


## Fan-in Scenario

Publisher pool: 3000

Subscriber pool: 1

Test durition: 5min

RPS: 1

QoS Level: 2

In Fan-in test 3000 clients publishing to exclusive channel subtopic, and single subscriber consuming all sent messages.
Both publishers and subcriber are connected with QoS 2, which is the safest and most demanding quality of service level that guarantees that each message is received exactly once by the intended recipients by using a four-part handshake.

No message loss was detected.

**NOTE:** Graphs Time unit is microsecond.

![DO fan-in 3k QoS 2 Total connections](assets/DO/fan-in-3k-qos2-totalconn.png "DO fan-in 3k QoS 2 Total connections")

*We see total number of connections during test is constant*

---

![DO fan-in 3k QoS 2 Total messages PRS](assets/DO/fan-in-3k-qos2-totalmsg.png "DO fan-in 3k QoS 2 Total sent messages")
![DO fan-in 3k QoS 2 Total received messages](assets/DO/fan-in-3k-qos2-totalmsg-rec.png "DO fan-in 3k QoS 2 Total received messages")


*We see total number of published and consumed messages is the same*

---

![DO fan-in 3k QoS 2 Total messages RPS](assets/DO/fan-in-3k-qos2-totalmsg-rps.png "DO fan-in 3k QoS 2 Total messages RPS")

*We see total number of messages per second (RPS) is 3000 for both publishing and receiving*

---

![DO fan-in 3k QoS 2 Publish to Pubrec latency](assets/DO/fan-in-3k-qos2-publishers-pub-pubrec-latency.png "DO fan-in 3k QoS 2 Pub to Sub latency")

*Pub to Pubrec latency in microseconds (100k microseconds are 100 milliseconds) *

---
> Useful facts: 
>* 95% of clients had latency up to 85ms
>* Max latency was up to 292ms

---

![DO fan-in 3k QoS 2 Pub to Sub latency](assets/DO/fan-in-3k-qos2-pub-sub-lat.png "DO fan-in 3k QoS 2 Pub to Sub latency")

*Pub to sub latency in microseconds (100k microseconds are 100 milliseconds) *

---
> Useful facts: 
>* 95% of clients had latency up to 1660ms
>* Max latency was up to 1829ms

---


Results metrics
![DO fan-in 3k QoS 2 Publish reveived ACK latency](assets/DO/fan-in-3k-qos2-results.png "DO fan-in 3k QoS 2 Publish reveived ACK")

*Results metrics are generated by mzbench tool*

---

### Kubernetes Cluster resources insights
![DO fan-in 3k QoS 2 - CPU usage on namespace](assets/DO/fan-in-3k-qos2-cpu-usage-namespace-wo-jaeger.png "DO fan-in 3k QoS 2 - CPU usage on namespace")

*Kubernetes CPU usage in whole Mainflux namespace during testing*

---

![DO fan-in 3k QoS 2 - Memory usage on namespace](assets/DO/fan-in-3k-qos2-mem-usage-namespace-wo-jaeger.png "DO fan-in 3k QoS 2 - Memory usage on namespace")

*Kubernetes Memory usage in whole Mainflux namespace during testing*

---

![DO fan-in 3k QoS 2 - CPU usage on MQTT statefulset](assets/DO/fan-in-3k-qos2-cpu-mqtt-adapters.png "DO fan-in 3k QoS 2 - CPU usage on MQTT statefulset")

*Kubernetes CPU usage in MQTT statefulset during testing*

---

![DO fan-in 3k QoS 2 - Memory usage on MQTT statefulset](assets/DO/fan-in-3k-qos2-mem-usage-mqtt.png "DO fan-in 3k QoS 2 - Memory usage on MQTT statefulset")

*Kubernetes Memory usage in MQTT statefulset during testing*

---

![DO fan-in 3k QoS 2 - Networking on MQTT statefulset](assets/DO/fan-in-3k-qos2-networking-mqtt.png "DO fan-in 3k QoS 2 - Networking on MQTT statefulset")

*Kubernetes Network receive/transmit bandwidth in MQTT statefulset during testing*

